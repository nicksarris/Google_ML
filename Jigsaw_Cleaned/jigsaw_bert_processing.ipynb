{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "__author__ = 'Nick Sarris (ngs5st)'\n",
    "\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification,BertAdam\n",
    "from pytorch_pretrained_bert.modeling import BertPreTrainedModel, BertModel\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "print(os.listdir(\"./data/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1235):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(1235)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Establishing Global Variables ...\")\n",
    "\n",
    "# Data Directory\n",
    "directory = './data/'\n",
    "\n",
    "# Torch Device\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# Model Parameters\n",
    "max_length = 220\n",
    "batch_size = 64\n",
    "n_epochs = 2\n",
    "accumulation_steps = 1\n",
    "\n",
    "# Model/Split Seed/Parameters\n",
    "# Change model_seed with every new/different model\n",
    "# Keep split_seed the same throughout\n",
    "model_seed = 1234\n",
    "current_split = 0\n",
    "\n",
    "# Model File Paths\n",
    "TRAIN_FILE = directory + 'train.csv'\n",
    "TEST_FILE  = directory + 'test.csv'\n",
    "PROCESSED_FILE = 'train_seq.pickle'\n",
    "\n",
    "# Directory/BERT Paths\n",
    "WORK_DIR = directory\n",
    "BERT_MODEL_PATH = directory + 'uncased_L-12_H-768_A-12/'\n",
    "BERT_WEIGHT_PATH = 'bert_pytorch_model.bin'\n",
    "\n",
    "convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n",
    "    BERT_MODEL_PATH + 'bert_model.ckpt',\n",
    "    BERT_MODEL_PATH + 'bert_config.json',\n",
    "    WORK_DIR + 'pytorch_model.bin')\n",
    "\n",
    "shutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')\n",
    "bert_config = BertConfig(BERT_MODEL_PATH + 'bert_config.json')\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lines(example, max_seq_length,tokenizer):\n",
    "    \n",
    "    max_seq_length -= 2\n",
    "    all_tokens = []\n",
    "    longer = 0\n",
    "    \n",
    "    for text in tqdm(example):\n",
    "        tokens_a = tokenizer.tokenize(text)\n",
    "        if len(tokens_a)>max_seq_length:\n",
    "            tokens_a = tokens_a[:max_seq_length]\n",
    "            longer += 1\n",
    "        one_token = (tokenizer.convert_tokens_to_ids(\n",
    "            [\"[CLS]\"] + tokens_a + [\"[SEP]\"]) + [0] * (max_seq_length - len(tokens_a)))\n",
    "        all_tokens.append(one_token)\n",
    "    \n",
    "    return np.array(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Processing Data ...\")\n",
    "\n",
    "bert_train = pd.read_csv(TRAIN_FILE)\n",
    "bert_train['comment_text'] = bert_train['comment_text'].astype(str) \n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_PATH, cache_dir=None, do_lower_case=True)\n",
    "sequences = convert_lines(bert_train[\"comment_text\"].fillna(\"DUMMY_VALUE\"), max_length, tokenizer)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Processing Data ...\")\n",
    "\n",
    "pickle_out = open(\"train_seq.pickle\",\"wb\")\n",
    "pickle.dump(sequences, pickle_out)\n",
    "pickle_out.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
