{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "__author__ = 'Nick Sarris (ngs5st)'\n",
    "\n",
    "import os\n",
    "import time\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import shutil\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "from random import shuffle\n",
    "\n",
    "from pytorch_pretrained_bert import convert_tf_checkpoint_to_pytorch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert.modeling import BertPreTrainedModel, BertModel\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "from apex import amp\n",
    "from apex.optimizers import FusedAdam\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tqdm.pandas()\n",
    "print(os.listdir(\"./data/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1235):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(1235)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBertClassifier(BertPreTrainedModel):\n",
    "\n",
    "    def __init__(self, config, num_aux_targets):\n",
    "        super(MyBertClassifier, self).__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.linear_out = nn.Linear(config.hidden_size, 1)\n",
    "        self.linear_aux_out = nn.Linear(config.hidden_size, num_aux_targets)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        \n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        \n",
    "        h_conc_linear1  = F.relu(self.linear(pooled_output))\n",
    "        h_conc_linear1 = self.dropout(h_conc_linear1)\n",
    "    \n",
    "        hidden = pooled_output + h_conc_linear1        \n",
    "        result = self.linear_out(hidden)\n",
    "        aux_result = self.linear_aux_out(hidden)\n",
    "        out = torch.cat([result, aux_result], 1)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Establishing Global Variables ...\")\n",
    "\n",
    "# Data Directory\n",
    "directory = './data/'\n",
    "\n",
    "# Torch Device\n",
    "device = torch.device('cuda')\n",
    "\n",
    "# Model Parameters\n",
    "max_length = 220\n",
    "batch_size = 64\n",
    "n_epochs = 2\n",
    "accumulation_steps = 1\n",
    "\n",
    "# Model/Split Seed/Parameters\n",
    "# Change model_seed with every new/different model\n",
    "# Keep split_seed the same throughout\n",
    "model_seed = 1234\n",
    "current_split = 0\n",
    "\n",
    "# Model File Paths\n",
    "TRAIN_FILE = directory + 'train.csv'\n",
    "TEST_FILE  = directory + 'test.csv'\n",
    "PROCESSED_FILE = 'train_seq.pickle'\n",
    "\n",
    "# Directory/BERT Paths\n",
    "WORK_DIR = directory\n",
    "BERT_MODEL_PATH = directory + 'uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/'\n",
    "BERT_WEIGHT_PATH = directory + 'bert_pytorch.bin'\n",
    "\n",
    "convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch(\n",
    "    BERT_MODEL_PATH + 'bert_model.ckpt',\n",
    "    BERT_MODEL_PATH + 'bert_config.json',\n",
    "    WORK_DIR + 'pytorch_model.bin')\n",
    "\n",
    "shutil.copyfile(BERT_MODEL_PATH + 'bert_config.json', WORK_DIR + 'bert_config.json')\n",
    "bert_config = BertConfig(BERT_MODEL_PATH + 'bert_config.json')\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Loading Initial Data ...\")\n",
    "\n",
    "with open(PROCESSED_FILE,'rb') as f:\n",
    "    x_train = pickle.load(f)\n",
    "\n",
    "lengths = torch.from_numpy(np.array([len(x) for x in x_train]))\n",
    "x_train = torch.tensor(pad_sequences(x_train, maxlen=max_length, padding='post'), dtype=torch.long)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Processing Initial Data ...\")\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_FILE)\n",
    "train_df['comment_text'] = train_df['comment_text'].astype(str) \n",
    "train_df = train_df.fillna(0)\n",
    "\n",
    "y_aux_train = train_df[['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']].values\n",
    "identity_columns = ['asian', 'atheist', 'bisexual', 'black', 'buddhist', 'christian', 'female', 'heterosexual', 'hindu', 'homosexual_gay_or_lesbian']\n",
    "\n",
    "weights = np.ones((len(train_df['comment_text']),)) / 4\n",
    "weights += (train_df[identity_columns].fillna(0).values >= 0.5).sum(axis=1) / 4\n",
    "weights += (( (train_df['target'].values>=0.5).astype(bool).astype(np.int) +\n",
    "    (train_df[identity_columns].fillna(0).values<0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "weights += (( (train_df['target'].values<0.5).astype(bool).astype(np.int) +\n",
    "    (train_df[identity_columns].fillna(0).values>=0.5).sum(axis=1).astype(bool).astype(np.int) ) > 1 ).astype(bool).astype(np.int) / 4\n",
    "\n",
    "y_train = np.vstack([(train_df['target'].values>=0.5).astype(np.int),weights]).T\n",
    "y_train_torch = torch.tensor(np.hstack([y_train, y_aux_train]), dtype=torch.float32)\n",
    "y_identities = (train_df[identity_columns] >= 0.5).astype(int).values\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceBucketCollator():\n",
    "    \n",
    "    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n",
    "        \n",
    "        self.choose_length = choose_length\n",
    "        self.sequence_index = sequence_index\n",
    "        self.length_index = length_index\n",
    "        self.label_index = label_index\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        \n",
    "        batch = [torch.stack(x) for x in list(zip(*batch))]\n",
    "        sequences = batch[self.sequence_index]\n",
    "        lengths = batch[self.length_index]\n",
    "        \n",
    "        length = self.choose_length(lengths)\n",
    "        mask = torch.arange(start=0, end=max_length, step=1) < length\n",
    "        padded_sequences = sequences[:, mask]\n",
    "        batch[self.sequence_index] = padded_sequences\n",
    "        \n",
    "        if self.label_index is not None:\n",
    "            return batch[self.sequence_index], batch[self.label_index]\n",
    "    \n",
    "        return batch[self.sequence_index]\n",
    "\n",
    "class LenMatchBatchSampler(data.BatchSampler):\n",
    "    \n",
    "    def __iter__(self):\n",
    "\n",
    "        buckets = [[]] * 100\n",
    "        yielded = 0\n",
    "\n",
    "        for idx in self.sampler:\n",
    "            count_zeros = torch.sum(self.sampler.data_source[idx][0] == 0)\n",
    "            count_zeros = int(count_zeros / 64) \n",
    "            if len(buckets[count_zeros]) == 0:  buckets[count_zeros] = []\n",
    "\n",
    "            buckets[count_zeros].append(idx)\n",
    "\n",
    "            if len(buckets[count_zeros]) == self.batch_size:\n",
    "                batch = list(buckets[count_zeros])\n",
    "                yield batch\n",
    "                yielded += 1\n",
    "                buckets[count_zeros] = []\n",
    "\n",
    "        batch = []\n",
    "        leftover = [idx for bucket in buckets for idx in bucket]\n",
    "\n",
    "        for idx in leftover:\n",
    "            batch.append(idx)\n",
    "            if len(batch) == self.batch_size:\n",
    "                yielded += 1\n",
    "                yield batch\n",
    "                batch = []\n",
    "\n",
    "        if len(batch) > 0 and not self.drop_last:\n",
    "            yielded += 1\n",
    "            yield batch\n",
    "\n",
    "        assert len(self) == yielded, \"produced an inccorect number of batches. expected %i, but yielded %i\" % (len(self), yielded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Finalizing Datasets ...\")\n",
    "    \n",
    "splits = list(KFold(n_splits=5, shuffle=True, random_state=model_seed)\n",
    "              .split(x_train, y_train_torch))\n",
    "\n",
    "train_idx = splits[current_split][0]\n",
    "valid_idx = splits[current_split][1]\n",
    "\n",
    "train_collator = SequenceBucketCollator(\n",
    "    lambda length_vals: length_vals.max(), sequence_index=0, length_index=1, label_index=2)\n",
    "test_collator = SequenceBucketCollator(\n",
    "    lambda length_vals: length_vals.max(), sequence_index=0, length_index=1)\n",
    "\n",
    "y_train_id_set = y_identities[train_idx]\n",
    "y_valid_id_set = y_identities[valid_idx]\n",
    "\n",
    "train_dataset = data.TensorDataset(x_train[train_idx], lengths[train_idx], y_train_torch[train_idx])\n",
    "valid_dataset = data.TensorDataset(x_train[valid_idx], lengths[valid_idx], y_train_torch[valid_idx])\n",
    "valid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=train_collator, \n",
    "                               pin_memory=True)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self, weight=None, alpha=0.5, gamma=3, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.size_average = size_average\n",
    "        self.eps = 1e-12\n",
    "        self.weight = weight\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \n",
    "        p = torch.sigmoid(inputs)\n",
    "        pt = torch.where(targets > 0, p, 1 - p)\n",
    "        t = torch.ones_like(targets)\n",
    "        alpha = torch.where(targets > 0, self.alpha * t, (1 - self.alpha) * t)\n",
    "\n",
    "        one_tensor = torch.Tensor([1]).cuda()\n",
    "        loss = -alpha * (torch.pow((1 - pt), self.gamma)) * torch.log(torch.min(pt + self.eps, one_tensor))\n",
    "        \n",
    "        if self.weight is not None: loss = loss * self.weight\n",
    "        if self.size_average: loss = loss.mean()\n",
    "        else: loss = loss.sum()\n",
    "            \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(data, targets):\n",
    "\n",
    "    bce_loss_1 = FocalLoss(weight=targets[:,1:2])(data[:,:1],targets[:,:1])\n",
    "    bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,1:],targets[:,2:])\n",
    "    return (bce_loss_1) + bce_loss_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Establishing Models ...\")\n",
    "\n",
    "lrs = [2e-5, 1e-5]\n",
    "model = MyBertClassifier.from_pretrained(\"../working\",cache_dir=None, num_aux_targets=y_aux_train.shape[-1])\n",
    "\n",
    "model.zero_grad()\n",
    "model = model.to(device)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = FusedAdam(optimizer_grouped_parameters, lr=2e-5, bias_correction=False, max_grad_norm=1.0)\n",
    "model, optimizer = amp.initialize(model, optimizer,  keep_batchnorm_fp32=False, opt_level=\"O2\",loss_scale=\"dynamic\", verbosity=0)\n",
    "model = model.train()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Training Models ...\")\n",
    "\n",
    "tk = tqdm(range(n_epochs))\n",
    "for epoch in tk:\n",
    "\n",
    "    step = 0\n",
    "    avg_loss = 0.\n",
    "    avg_accuracy = 0.\n",
    "    lossf = None\n",
    "\n",
    "    lr = lrs[epoch]\n",
    "    seed_everything(model_seed + 2 * epoch + current_split)\n",
    "    ran_sampler = data.RandomSampler(train_dataset)\n",
    "    len_sampler = LenMatchBatchSampler(ran_sampler, batch_size=batch_size, drop_last = False)\n",
    "    train_loader = data.DataLoader(train_dataset, batch_sampler=len_sampler, collate_fn=train_collator, pin_memory=True)\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lrs[epoch]\n",
    "\n",
    "    tk0 = tqdm(enumerate(train_loader), total=len(train_loader), leave=False)    \n",
    "    warmup = WarmupLinearSchedule(warmup=0.05, t_total=len(train_loader))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    for i, (x_batch, y_batch) in tk0:\n",
    "\n",
    "        y_pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)    \n",
    "        loss = custom_loss(y_pred, y_batch.to(device))\n",
    "\n",
    "        with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "            scaled_loss.backward()\n",
    "        if (i+1) % accumulation_steps == 0:\n",
    "            lr_this_step = lr * warmup.get_lr(step, 0.05)\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr_this_step\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            step += 1\n",
    "\n",
    "        if lossf:\n",
    "            lossf = 0.98*lossf+0.02*loss.item()\n",
    "        else:\n",
    "            lossf = loss.item()\n",
    "\n",
    "        tk0.set_postfix(loss = lossf)        \n",
    "        avg_loss += loss.item() / len(train_loader)\n",
    "        avg_accuracy += torch.mean(((torch.sigmoid(y_pred[:,0])>0.5) == (y_batch[:,0]>0.5).to(device)).to(torch.float) ).item()/len(train_loader)\n",
    "    \n",
    "    tk.set_postfix(avg_loss=avg_loss,avg_accuracy=avg_accuracy)\n",
    "    output_model_file = \"bert_pytorch_model_{}.bin\".format(epoch)\n",
    "    torch.save(model.state_dict(), output_model_file)    \n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "\n",
    "    def __init__(self, y_true, y_identity, power=-5, overall_model_weight=0.25):\n",
    "        self.y = (y_true >= 0.5).astype(int)\n",
    "        self.y_i = (y_identity >= 0.5).astype(int)\n",
    "        self.n_subgroups = self.y_i.shape[1]\n",
    "        self.power = power\n",
    "        self.overall_model_weight = overall_model_weight\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_auc(y_true, y_pred):\n",
    "        try:\n",
    "            return roc_auc_score(y_true, y_pred)\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "\n",
    "    def _compute_subgroup_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] == 1\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "\n",
    "    def _compute_bpsn_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] + self.y == 1\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "\n",
    "    def _compute_bnsp_auc(self, i, y_pred):\n",
    "        mask = self.y_i[:, i] + self.y != 1\n",
    "        return self._compute_auc(self.y[mask], y_pred[mask])\n",
    "\n",
    "    def compute_bias_metrics_for_model(self, y_pred):\n",
    "        records = np.zeros((3, self.n_subgroups))\n",
    "        for i in range(self.n_subgroups):\n",
    "            records[0, i] = self._compute_subgroup_auc(i, y_pred)\n",
    "            records[1, i] = self._compute_bpsn_auc(i, y_pred)\n",
    "            records[2, i] = self._compute_bnsp_auc(i, y_pred)\n",
    "        return records\n",
    "\n",
    "    def _calculate_overall_auc(self, y_pred):\n",
    "        return roc_auc_score(self.y, y_pred)\n",
    "\n",
    "    def _power_mean(self, array):\n",
    "        total = sum(np.power(array, self.power))\n",
    "        return np.power(total / len(array), 1 / self.power)\n",
    "\n",
    "    def get_final_metric(self, y_pred):\n",
    "        bias_metrics = self.compute_bias_metrics_for_model(y_pred)\n",
    "        bias_score = np.average([\n",
    "            self._power_mean(bias_metrics[0]),\n",
    "            self._power_mean(bias_metrics[1]),\n",
    "            self._power_mean(bias_metrics[2])\n",
    "        ])\n",
    "        overall_score = self.overall_model_weight * self._calculate_overall_auc(y_pred)\n",
    "        bias_score = (1 - self.overall_model_weight) * bias_score\n",
    "        return overall_score + bias_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Validating Models ...\")\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.eval()\n",
    "valid_preds = np.zeros((len(y_train_torch[valid_idx])))\n",
    "tk1 = tqdm(enumerate(valid_loader), total=len(valid_loader), leave=False)\n",
    "evaluator = Evaluator(np.array(y_train_torch[valid_idx])[:, 0], y_valid_id_set)\n",
    "\n",
    "for i, (x_batch, y_batch) in tk1:\n",
    "\n",
    "    y_pred = model(x_batch.to(device), attention_mask=(x_batch > 0).to(device), labels=None)\n",
    "    valid_preds[i * batch_size:(i + 1) * batch_size] = y_pred[:,0].detach().cpu().squeeze().numpy()\n",
    "    \n",
    "    if i % 200 == 0:\n",
    "        print(\"Model: 1 | Current CV Score: {}\".format(evaluator.get_final_metric(valid_preds)))\n",
    "\n",
    "valid_preds = torch.sigmoid(torch.tensor(valid_preds)).numpy().ravel()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model: 1 | Final CV Score: {}\".format(evaluator.get_final_metric(valid_preds)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
