{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "__author__ = 'Nick Sarris (ngs5st)'\n",
    "\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as ctb\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras import backend as K\n",
    "\n",
    "print(os.listdir(\"./data/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=1235):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(1235)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Loading Data ...\")\n",
    "\n",
    "directory = \"./data/\"\n",
    "train_df = pd.read_csv(directory + 'train.csv')\n",
    "test_df = pd.read_csv(directory + 'test.csv')\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Feature Engineering [1] ...\")\n",
    "\n",
    "train_df['pickup_datetime'] = pd.to_datetime(train_df.pickup_datetime)\n",
    "test_df['pickup_datetime'] = pd.to_datetime(test_df.pickup_datetime)\n",
    "train_df['dropoff_datetime'] = pd.to_datetime(train_df.dropoff_datetime)\n",
    "train_df['store_and_fwd_flag'] = 1 * (train_df.store_and_fwd_flag.values == 'Y')\n",
    "test_df['store_and_fwd_flag'] = 1 * (test_df.store_and_fwd_flag.values == 'Y')\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Feature Engineering [2] ...\")\n",
    "\n",
    "train_df.loc[:, 'pickup_date'] = train_df['pickup_datetime'].dt.date\n",
    "train_df.loc[:, 'pickup_weekday'] = train_df['pickup_datetime'].dt.weekday\n",
    "train_df.loc[:, 'pickup_day'] = train_df['pickup_datetime'].dt.day\n",
    "train_df.loc[:, 'pickup_month'] = train_df['pickup_datetime'].dt.month\n",
    "train_df.loc[:, 'pickup_hour'] = train_df['pickup_datetime'].dt.hour\n",
    "train_df.loc[:, 'pickup_minute'] = train_df['pickup_datetime'].dt.minute\n",
    "train_df.loc[:, 'pickup_dt'] = (train_df['pickup_datetime'] - train_df['pickup_datetime'].min()).map(\n",
    "    lambda x: x.total_seconds())\n",
    "\n",
    "test_df.loc[:, 'pickup_date'] = test_df['pickup_datetime'].dt.date\n",
    "test_df.loc[:, 'pickup_weekday'] = test_df['pickup_datetime'].dt.weekday\n",
    "test_df.loc[:, 'pickup_day'] = test_df['pickup_datetime'].dt.day\n",
    "test_df.loc[:, 'pickup_month'] = test_df['pickup_datetime'].dt.month\n",
    "test_df.loc[:, 'pickup_hour'] = test_df['pickup_datetime'].dt.hour\n",
    "test_df.loc[:, 'pickup_minute'] = test_df['pickup_datetime'].dt.minute\n",
    "test_df.loc[:, 'pickup_dt'] = (test_df['pickup_datetime'] - train_df['pickup_datetime'].min()).map(\n",
    "    lambda x: x.total_seconds())\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine_array(lat1, lng1, lat2, lng2):\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    AVG_EARTH_RADIUS = 6371  # in km\n",
    "    lat = lat2 - lat1\n",
    "    lng = lng2 - lng1\n",
    "    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n",
    "    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n",
    "    return h\n",
    "\n",
    "def dummy_manhattan_distance(lat1, lng1, lat2, lng2):\n",
    "    a = haversine_array(lat1, lng1, lat1, lng2)\n",
    "    b = haversine_array(lat1, lng1, lat2, lng1)\n",
    "    return a + b\n",
    "\n",
    "def bearing_array(lat1, lng1, lat2, lng2):\n",
    "    AVG_EARTH_RADIUS = 6371  # in km\n",
    "    lng_delta_rad = np.radians(lng2 - lng1)\n",
    "    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n",
    "    y = np.sin(lng_delta_rad) * np.cos(lat2)\n",
    "    x = np.cos(lat1) * np.sin(lat2) - np.sin(lat1) * np.cos(lat2) * np.cos(lng_delta_rad)\n",
    "    return np.degrees(np.arctan2(y, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Feature Engineering [3] ...\")\n",
    "\n",
    "train_df['distance_haversine'] = haversine_array(\n",
    "    train_df['pickup_latitude'].values, train_df['pickup_longitude'].values,\n",
    "    train_df['dropoff_latitude'].values, train_df['dropoff_longitude'].values)\n",
    "\n",
    "train_df['distance_dummy_manhattan'] = dummy_manhattan_distance(\n",
    "    train_df['pickup_latitude'].values, train_df['pickup_longitude'].values,\n",
    "    train_df['dropoff_latitude'].values, train_df['dropoff_longitude'].values)\n",
    "\n",
    "test_df['distance_haversine'] = haversine_array(\n",
    "    test_df['pickup_latitude'].values, test_df['pickup_longitude'].values,\n",
    "    test_df['dropoff_latitude'].values, test_df['dropoff_longitude'].values)\n",
    "\n",
    "test_df['distance_dummy_manhattan'] = dummy_manhattan_distance(\n",
    "    test_df['pickup_latitude'].values, test_df['pickup_longitude'].values,\n",
    "    test_df['dropoff_latitude'].values, test_df['dropoff_longitude'].values)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Feature Engineering [4] ...\")\n",
    "\n",
    "train_df['center_latitude'] = (train_df['pickup_latitude'].values + train_df['dropoff_latitude'].values) / 2\n",
    "train_df['center_longitude'] = (train_df['pickup_longitude'].values + train_df['dropoff_longitude'].values) / 2\n",
    "test_df['center_latitude'] = (test_df['pickup_latitude'].values + test_df['dropoff_latitude'].values) / 2\n",
    "test_df['center_longitude'] = (test_df['pickup_longitude'].values + test_df['dropoff_longitude'].values) / 2\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Feature Engineering [5] ...\")\n",
    "\n",
    "train_df['pickup_lat_bin'] = np.round(train_df['pickup_latitude'], 2)\n",
    "train_df['pickup_long_bin'] = np.round(train_df['pickup_longitude'], 2)\n",
    "train_df['center_lat_bin'] = np.round(train_df['center_latitude'], 2)\n",
    "train_df['center_long_bin'] = np.round(train_df['center_longitude'], 2)\n",
    "train_df['pickup_dt_bin'] = (train_df['pickup_dt'] // (3 * 3600))\n",
    "test_df['pickup_lat_bin'] = np.round(test_df['pickup_latitude'], 2)\n",
    "test_df['pickup_long_bin'] = np.round(test_df['pickup_longitude'], 2)\n",
    "test_df['center_lat_bin'] = np.round(test_df['center_latitude'], 2)\n",
    "test_df['center_long_bin'] = np.round(test_df['center_longitude'], 2)\n",
    "test_df['pickup_dt_bin'] = (test_df['pickup_dt'] // (3 * 3600))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Feature Engineering [6] ...\")\n",
    "\n",
    "train_df.loc[:, 'direction'] = bearing_array(\n",
    "    train_df['pickup_latitude'].values, train_df['pickup_longitude'].values,\n",
    "    train_df['dropoff_latitude'].values, train_df['dropoff_longitude'].values)\n",
    "\n",
    "test_df.loc[:, 'direction'] = bearing_array(\n",
    "    test_df['pickup_latitude'].values, test_df['pickup_longitude'].values,\n",
    "    test_df['dropoff_latitude'].values, test_df['dropoff_longitude'].values)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Feature Engineering [7] ...\")\n",
    "\n",
    "full = pd.concat([train_df, test_df]).reset_index(drop=True)\n",
    "coords = np.vstack((full[['pickup_latitude', 'pickup_longitude']],\n",
    "                    full[['dropoff_latitude', 'dropoff_longitude']]))\n",
    "\n",
    "pca = PCA().fit(coords)\n",
    "train_df['pickup_pca0'] = pca.transform(train_df[['pickup_latitude', 'pickup_longitude']])[:, 0]\n",
    "train_df['pickup_pca1'] = pca.transform(train_df[['pickup_latitude', 'pickup_longitude']])[:, 1]\n",
    "train_df['dropoff_pca0'] = pca.transform(train_df[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\n",
    "train_df['dropoff_pca1'] = pca.transform(train_df[['dropoff_latitude', 'dropoff_longitude']])[:, 1]\n",
    "test_df['pickup_pca0'] = pca.transform(test_df[['pickup_latitude', 'pickup_longitude']])[:, 0]\n",
    "test_df['pickup_pca1'] = pca.transform(test_df[['pickup_latitude', 'pickup_longitude']])[:, 1]\n",
    "test_df['dropoff_pca0'] = pca.transform(test_df[['dropoff_latitude', 'dropoff_longitude']])[:, 0]\n",
    "test_df['dropoff_pca1'] = pca.transform(test_df[['dropoff_latitude', 'dropoff_longitude']])[:, 1]\n",
    "\n",
    "kmeans = MiniBatchKMeans(n_clusters=100, batch_size=10000).fit(coords)\n",
    "train_df.loc[:, 'pickup_cluster'] = kmeans.predict(train_df[['pickup_latitude', 'pickup_longitude']])\n",
    "train_df.loc[:, 'dropoff_cluster'] = kmeans.predict(train_df[['dropoff_latitude', 'dropoff_longitude']])\n",
    "test_df.loc[:, 'pickup_cluster'] = kmeans.predict(test_df[['pickup_latitude', 'pickup_longitude']])\n",
    "test_df.loc[:, 'dropoff_cluster'] = kmeans.predict(test_df[['dropoff_latitude', 'dropoff_longitude']])\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Feature Engineering [8] ...\")\n",
    "\n",
    "group_freq = '60min'\n",
    "df_all = pd.concat((train_df, test_df))[['id', 'pickup_datetime', 'pickup_cluster', 'dropoff_cluster']]\n",
    "train_df.loc[:, 'pickup_datetime_group'] = train_df['pickup_datetime'].dt.round(group_freq)\n",
    "test_df.loc[:, 'pickup_datetime_group'] = test_df['pickup_datetime'].dt.round(group_freq)\n",
    "\n",
    "df_counts = df_all.set_index('pickup_datetime')[['id']].sort_index()\n",
    "df_counts['count_60min'] = df_counts.isnull().rolling(group_freq).count()['id']\n",
    "train_df = train_df.merge(df_counts, on='id', how='left')\n",
    "test_df = test_df.merge(df_counts, on='id', how='left')\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Feature Engineering [9] ...\")\n",
    "\n",
    "train_df['pca_manhattan'] = np.abs(train_df['dropoff_pca1'] - train_df['pickup_pca1']) + \\\n",
    "                         np.abs(train_df['dropoff_pca0'] - train_df['pickup_pca0'])\n",
    "\n",
    "test_df['pca_manhattan'] = np.abs(test_df['dropoff_pca1'] - test_df['pickup_pca1']) + \\\n",
    "                        np.abs(test_df['dropoff_pca0'] - test_df['pickup_pca0'])\n",
    "\n",
    "train_df['direction_ns'] = (train_df.pickup_latitude > train_df.dropoff_latitude) * 1 + 1\n",
    "indices = train_df[(train_df.pickup_latitude == train_df.dropoff_longitude) & (train_df.pickup_latitude != 0)].index\n",
    "train_df.loc[indices, 'direction_ns'] = 0\n",
    "\n",
    "train_df['direction_ew'] = (train_df.pickup_longitude > train_df.dropoff_longitude) * 1 + 1\n",
    "indices = train_df[(train_df.pickup_longitude == train_df.dropoff_longitude) & (train_df.pickup_longitude != 0)].index\n",
    "train_df.loc[indices, 'direction_ew'] = 0\n",
    "\n",
    "test_df['direction_ns'] = (test_df.pickup_latitude > test_df.dropoff_latitude) * 1 + 1\n",
    "indices = test_df[(test_df.pickup_latitude == test_df.dropoff_longitude) & (test_df.pickup_latitude != 0)].index\n",
    "test_df.loc[indices, 'direction_ns'] = 0\n",
    "\n",
    "test_df['direction_ew'] = (test_df.pickup_longitude > test_df.dropoff_longitude) * 1 + 1\n",
    "indices = test_df[(test_df.pickup_longitude == test_df.dropoff_longitude) & (test_df.pickup_longitude != 0)].index\n",
    "test_df.loc[indices, 'direction_ew'] = 0\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Processing Features ...\")\n",
    "\n",
    "cols_to_drop = ['id', 'log_trip_duration', 'pickup_datetime', 'dropoff_datetime',\n",
    "                'trip_duration', 'check_trip_duration', 'pickup_date', 'pickup_lat_bin', 'pickup_long_bin',\n",
    "                'center_lat_bin', 'center_long_bin', 'pickup_dt_bin', 'pickup_datetime_group']\n",
    "\n",
    "ids = test_df[\"id\"].values\n",
    "labels = np.log(train_df['trip_duration'].values + 1)\n",
    "features = [f for f in train_df.columns if f not in cols_to_drop]\n",
    "\n",
    "train_df = train_df[features]\n",
    "test_df = test_df[features]\n",
    "\n",
    "for f in train_df.columns:\n",
    "    if train_df[f].dtype == 'object':\n",
    "        lbl = preprocessing.LabelEncoder()\n",
    "        lbl.fit(list(train_df[f].values))\n",
    "        train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "        test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XgbWrapper(object):\n",
    "    \n",
    "    def __init__(self, seed=2017, params=None):\n",
    "        self.param = params\n",
    "        self.param['seed'] = seed\n",
    "\n",
    "    def train(self, xtra, ytra, xte, yte):\n",
    "        dtrain = xgb.DMatrix(xtra, label=ytra)\n",
    "        dvalid = xgb.DMatrix(xte, label=yte)\n",
    "        watchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "        self.gbdt = xgb.train(self.param, dtrain, 200,\n",
    "            watchlist, early_stopping_rounds=10, verbose_eval=20)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.gbdt.predict(xgb.DMatrix(x))\n",
    "\n",
    "class LgbWrapper(object):\n",
    "    \n",
    "    def __init__(self, seed=2017, params=None):\n",
    "        self.param = params\n",
    "        self.param['seed'] = seed\n",
    "\n",
    "    def train(self, xtra, ytra, xte, yte):\n",
    "        ytra = ytra.ravel()\n",
    "        yte = yte.ravel()\n",
    "        dtrain = lgb.Dataset(xtra, label=ytra)\n",
    "        dvalid = lgb.Dataset(xte, label=yte)\n",
    "        watchlist = [dvalid]\n",
    "        self.gbdt = lgb.train(self.param, dtrain, 400, \n",
    "            watchlist, early_stopping_rounds=10, verbose_eval=20)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.gbdt.predict(x)\n",
    "\n",
    "class CtbWrapper(object):\n",
    "    \n",
    "    def __init__(self, seed=2017, params=None):\n",
    "        self.seed = seed\n",
    "\n",
    "    def train(self, xtra, ytra, xte, yte):\n",
    "        self.gbdt = ctb.CatBoostRegressor(depth=14,\n",
    "            iterations=250, random_seed=self.seed,\n",
    "            use_best_model=True, loss_function='RMSE',\n",
    "            thread_count=16, eval_metric='RMSE')\n",
    "\n",
    "        xtra = pd.DataFrame(xtra)\n",
    "        ytra = pd.DataFrame(ytra)\n",
    "        xte = pd.DataFrame(xte)\n",
    "        yte = pd.DataFrame(yte)\n",
    "\n",
    "        self.gbdt.fit(X=xtra, y=ytra, eval_set=(xte, yte),\n",
    "                      use_best_model=True, verbose_eval=20)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.gbdt.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oof(clf, ntrain, ntest, kf, train, labels, test):\n",
    "\n",
    "    oof_train = np.zeros((ntrain,))\n",
    "    oof_test = np.zeros((ntest,))\n",
    "    oof_test_skf = np.empty((5, ntest))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        x_tr = train[train_index]\n",
    "        y_tr = labels[train_index]\n",
    "        x_te = train[test_index]\n",
    "        y_te = labels[test_index]\n",
    "\n",
    "        clf.train(x_tr, y_tr, x_te, y_te)\n",
    "        oof_train[test_index] = clf.predict(x_te)\n",
    "        oof_test_skf[i, :] = clf.predict(test)\n",
    "\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Splitting Data ...\")\n",
    "\n",
    "train_x = np.array(train_df)\n",
    "test_x = np.array(test_df)\n",
    "labels = np.array(labels)\n",
    "\n",
    "ntrain = train_x.shape[0]\n",
    "ntest = test_x.shape[0]\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Establishing Parameters ...\")\n",
    "\n",
    "lgb_params = {}\n",
    "lgb_params['boosting_type'] = 'gbdt'\n",
    "lgb_params['objective'] = 'regression'\n",
    "lgb_params['metric'] = 'mse'\n",
    "lgb_params['num_leaves'] = 96\n",
    "lgb_params['max_depth'] = 10\n",
    "lgb_params['feature_fraction'] = 0.9\n",
    "lgb_params['bagging_fraction'] = 0.95\n",
    "lgb_params['bagging_freq'] = 5\n",
    "lgb_params['learning_rate'] = 0.1\n",
    "lgb_params['early_stopping_round'] = 20\n",
    "\n",
    "xgb_params = {}\n",
    "xgb_params['booster'] = 'gbtree'\n",
    "xgb_params['objective'] = 'reg:linear'\n",
    "xgb_params['learning_rate'] = 0.1\n",
    "xgb_params['max_depth'] = 14\n",
    "xgb_params['subsample'] = 0.8\n",
    "xgb_params['colsample_bytree'] = 0.7\n",
    "xgb_params['colsample_bylevel'] = 0.7\n",
    "xgb_params['silent'] = 1\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Training Catboost ...\")\n",
    "\n",
    "cg = CtbWrapper()\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=2018).split(train_x)\n",
    "cg_oof_train, cg_oof_test = get_oof(cg, ntrain, ntest, kf, train_x, labels, test_x)\n",
    "print(\"CG-CV: {}\".format(mean_squared_error(labels, cg_oof_train)))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Training XGBoost ...\")\n",
    "\n",
    "xg = XgbWrapper(seed=2017, params=xgb_params)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=2018).split(train_x)\n",
    "xg_oof_train, xg_oof_test = get_oof(xg, ntrain, ntest, kf, train_x, labels, test_x)\n",
    "print(\"XG-CV: {}\".format(mean_squared_error(labels, xg_oof_train)))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Training LightGBM ...\")\n",
    "\n",
    "lg = LgbWrapper(seed=2017, params=lgb_params)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=2018).split(train_x)\n",
    "lg_oof_train, lg_oof_test = get_oof(lg, ntrain, ntest, kf, train_x, labels, test_x)\n",
    "print(\"LG-CV: {}\".format(mean_squared_error(labels, lg_oof_train)))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Combining Data ...\")\n",
    "\n",
    "train_conc = np.concatenate((cg_oof_train, xg_oof_train, lg_oof_train), axis=1)\n",
    "test_conc = np.concatenate((cg_oof_test, xg_oof_test, lg_oof_test), axis=1)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Training Stacking Ensemble ...\")\n",
    "\n",
    "dtrain = xgb.DMatrix(train_conc, label=labels)\n",
    "dtest = xgb.DMatrix(test_conc)\n",
    "\n",
    "xgb_params = {}\n",
    "xgb_params[\"objective\"] = \"reg:linear\"\n",
    "xgb_params[\"eta\"] = 0.1\n",
    "xgb_params[\"subsample\"] = 0.9\n",
    "xgb_params[\"silent\"] = 1\n",
    "xgb_params[\"max_depth\"] = 5\n",
    "xgb_params['eval_metric'] = 'rmse'\n",
    "xgb_params['min_child_weight'] = 10\n",
    "xgb_params['seed'] = 2017\n",
    "\n",
    "res = xgb.cv(xgb_params, dtrain, num_boost_round=500, nfold=5, seed=2017, stratified=False,\n",
    "             early_stopping_rounds=25, verbose_eval=10, show_stdv=True)\n",
    "\n",
    "best_nrounds = res.shape[0] - 1\n",
    "cv_mean = res.iloc[-1, 0]\n",
    "cv_std = res.iloc[-1, 1]\n",
    "\n",
    "print('Ensemble-CV: {0}+{1}'.format(cv_mean, cv_std))\n",
    "bst = xgb.train(xgb_params, dtrain, best_nrounds)\n",
    "stacking_preds = np.exp(bst.predict(dtest))\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=400, input_dim=28, kernel_initializer='uniform', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.6))\n",
    "\n",
    "    model.add(Dense(units=100, kernel_initializer='uniform', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.4))\n",
    "\n",
    "    model.add(Dense(units=1, kernel_initializer='uniform', activation='relu'))\n",
    "    model.compile(loss='mean_squared_error', optimizer='sgd')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Preparing Data for NN ...\")\n",
    "\n",
    "tr_te = pd.concat([train_df, test_df]).reset_index(drop=True)\n",
    "\n",
    "ntrain = train_df.shape[0]\n",
    "ntest = test_df.shape[0]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "tr_te = scaler.fit_transform(tr_te)\n",
    "\n",
    "train_x = tr_te[:ntrain, :]\n",
    "test_x = tr_te[ntrain:, :]\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Training Neural Net ...\")\n",
    "\n",
    "model = nn_model()\n",
    "model.fit(train_x, labels, batch_size=64, epochs=20)\n",
    "neural_preds = model.predict(test_x)[:, 0]\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Generate Submission ...\")\n",
    "\n",
    "combined_preds = (stacking_preds * 0.85) + (neural_preds * 0.15)\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['id'] = ids\n",
    "submission['trip_duration'] = combined_preds\n",
    "submission.to_csv(\"output_data.csv\", index=False)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
